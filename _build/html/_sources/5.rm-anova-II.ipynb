{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bde1ccab-2d89-4ccf-aca6-dbea239a0625",
   "metadata": {},
   "source": [
    "# Higher-order Repeated Measures ANOVA\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0025798-daa1-475d-b3f3-ed1ac8da1a0c",
   "metadata": {},
   "source": [
    "## Adding Between-subjects Factors\n",
    "\n",
    "... So what is the correct error here? The obvious, and correct, answer is that it is the *between-subjects* error. So how do we use this in our test statistics? At present, all we have done is *removed* the between-subjects error by including the `Subject` factor in our models. However, we somehow have to use this removed error as the denominator in tests that are based on between-subjects effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cee845f",
   "metadata": {},
   "source": [
    "## Add More Within-subject Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1eb7ed",
   "metadata": {},
   "source": [
    "### Using the `ezANOVA()` Function\n",
    "As we can see above, using a partitioned error model with `aov()` is a tricky business and it would be very easy to get this wrong. As an alternative, we can use the `ezANOVA()` function from the `ez` package. As the name implies, this is designed to allow for an RM ANOVA without the usual difficulties associated with the `aov()` or `lm()` functions. Unfortuantely, the aim of this package is largely to make the `R` output the same as SPSS. So it does away with the linear model framework. This means, no residuals, no parameter estimates, no diagnostic plots or anything else we have made use of so far. If you *have* to use an RM ANOVA, this is the simplest way to get it *right*. However, as we will discuss below, we would disuade you from ever considering RM ANOVA as an option in the future. About the only utility of this is showing doubtful researchers that our better options of GLS and mixed-effects models are, in fact, giving them the same answer as an RM ANOVA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdee81e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "\u001b[1m\u001b[33mError\u001b[39m in `ezANOVA()`:\u001b[22m\n\u001b[33m!\u001b[39m argument \"data\" is missing, with no default",
     "output_type": "error",
     "traceback": [
      "\u001b[1m\u001b[33mError\u001b[39m in `ezANOVA()`:\u001b[22m\n",
      "\u001b[33m!\u001b[39m argument \"data\" is missing, with no default\n",
      "\u001b[90m    \u001b[39m▆\n",
      "\u001b[90m 1. \u001b[39m└─\u001b[1mez\u001b[22m::ezANOVA()"
     ]
    }
   ],
   "source": [
    "library(ez)\n",
    "ezANOVA()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fdc59d",
   "metadata": {},
   "source": [
    "## Why We Should *Not* Use RM ANOVA\n",
    "Everything we have discussed above has really been an exercise in telling you why you really do not want to use RM ANOVA. All the unncessary fiddling with error terms and different tests requiring different errors is a complication that we could simply do without. Even if we do manage to successfully work out what needs to go where (or get a function like `ezANOVA()` to sort it for us), we are still left with a method that has a number of meaningful restrictions. ... Because of this, the RM ANOVA is both tricky to understand, tricky to use correctly and massively inflexible. It is no wonder that statisticians abandoned this method decades ago! And yet, this is the method that has persisted in psychology until releatively recently.\n",
    "\n",
    "... testing assumptions and follow-up tests...\n",
    "\n",
    "This section has largely been motivational to understand why we want to use something more flexible and more modern, but it is important to recognise that you may well end up working with someone who knows nothing beyond the RM ANOVA. In those situations, it is useful to (a) motivate the need for something better and (b) understand how to get the RM ANOVA results in `R`, in case they require further convincing. So, we do not condone the use of the RM ANOVA, but we understand its place in psychology and also understand that there are times where you may want to see what the RM ANOVA says, even if you do not wish to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b66d2d9-9c81-478c-a0d9-6b8ed9add05a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8dc6312",
   "metadata": {},
   "source": [
    "[^submodel-foot]: An alternative perspective here is that each error term represents a different *sub-model*. So, we can think of specifying *multiple* models, some of which require us to *average-over* certain factors. For instance, if we were to average-over the repeated measurements and then fit a model on the resultant outcome variable, this model would automatically have $\\sigma^{2}_{b}$ as its error term. This does make the whole procedure feel a little bit less of a hack, however, it is very impractical to do this, especially when the number of factors and interactions gets larger. You can read more about this approach in [McFarquhar (2019)](https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2019.00352/full).\n",
    "\n",
    "[^noterr-foot]: Note that this is *not* the errors from the linear model, even though the Greek letter is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e895875",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "name": "r"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
