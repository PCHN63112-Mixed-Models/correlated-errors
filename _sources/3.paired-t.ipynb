{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bde1ccab-2d89-4ccf-aca6-dbea239a0625",
   "metadata": {},
   "source": [
    "# The Paired *t*-test\n",
    "To begin our journey into the world of models for repeated measurements, we will start with the simplest possible example: the paired $t$-test. Last semester, we saw how the humble $t$-test can be subsumed into the linear model framework through the use of *dummy variables*. This same approach can be used with paired data[^paired-foot] because the difference between the *independent* and *paired* $t$-test does not lie with the *mean function*. We spent a lot of time last semester discussing different mean functions and, you will be glad to know, this all follows-over into the world of repeated measurements. The difference lies with the *variance function*. Specifically, how we can alter the variance function to accommodate non-zero correlation between the repeats? As we will see in this part of the lesson, there are *two* equivalent ways of doing this with paired data. One of them *side-steps* the issues of dependence whereas the other does not. However, it is useful to understand how both of these work because it provides some crucial insight into modelling repeated measurements that we will generalise as our models get more complex.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decc8b1f-545c-4f4b-abf6-505f93433003",
   "metadata": {},
   "source": [
    "## Two-sample vs Paired *t*-tests\n",
    "To begin with, it is useful to examine *how* the results differ between a *two-sample* and *paired* $t$-test. We can don this in `R` by comparing the results of the `t.test()` function with `paired=FALSE` and `paired=TRUE`. To do this, we use the `mice2` data set from the `datarium` package, that contains the weight of a sample of 10 mice both *before* and *after* some treatment. The experimental question concerns whether the treatment affects the weight of the mice. The data is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcc5306a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id before after\n",
      "1   1  187.2 429.5\n",
      "2   2  194.2 404.4\n",
      "3   3  231.7 405.6\n",
      "4   4  200.5 397.2\n",
      "5   5  201.7 377.9\n",
      "6   6  235.0 445.8\n",
      "7   7  208.7 408.4\n",
      "8   8  172.4 337.0\n",
      "9   9  184.6 414.3\n",
      "10 10  189.6 380.3\n"
     ]
    }
   ],
   "source": [
    "library('datarium')\n",
    "data('mice2')\n",
    "print(mice2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f81866",
   "metadata": {},
   "source": [
    "We can compare the output from a *two-sample* $t$-test and a *paired* $t$-test by changing the `paired=` argument of `t.test()`, as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d29a1f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tPaired t-test\n",
      "\n",
      "data:  mice2$before and mice2$after\n",
      "t = -25.546, df = 9, p-value = 1.039e-09\n",
      "alternative hypothesis: true mean difference is not equal to 0\n",
      "95 percent confidence interval:\n",
      " -217.1442 -181.8158\n",
      "sample estimates:\n",
      "mean difference \n",
      "        -199.48 \n",
      "\n",
      "\n",
      "\tTwo Sample t-test\n",
      "\n",
      "data:  mice2$before and mice2$after\n",
      "t = -17.453, df = 18, p-value = 9.974e-13\n",
      "alternative hypothesis: true difference in means is not equal to 0\n",
      "95 percent confidence interval:\n",
      " -223.4926 -175.4674\n",
      "sample estimates:\n",
      "mean of x mean of y \n",
      "   200.56    400.04 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(t.test(mice2$before, mice2$after, var.equal=TRUE, paired=TRUE))  # paired t-test\n",
    "print(t.test(mice2$before, mice2$after, var.equal=TRUE, paired=FALSE)) # two-sample t-test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fdd680",
   "metadata": {},
   "source": [
    "The output is a bit different between the two methods, so let us spend a little time unpacking what *is* and what *is not* different here. To begin with, the clearest differences between the two methods concern the $t$-statistic itself, the degrees of freedom, the $p$-value and the confidence interval. This is summarised in the table below \n",
    "\n",
    "| Test       | *t*-statistic | DoF | *p*-value | 95% CI            | \n",
    "| ---------- | ------------- | --- | --------- | ----------------- |\n",
    "| Paired     | -25.546       | 9   | 1.039e-09 | [-217.14 -181.82] |\n",
    "| Two-sample | -17.453       | 18  | 9.974e-13 | [-223.49 -175.47] |\n",
    "\n",
    "Although this may therefore seem like *everything* is different, there is actually one element that is *identical* here, though it is somewhat hidden. To see it, consider that the structure of a $t$-test is\n",
    "\n",
    "$$\n",
    "t = \\frac{\\mu_{1} - \\mu_{2}}{\\text{SE}\\{\\mu_{1} - \\mu_{2}\\}},\n",
    "$$\n",
    "\n",
    "meaning that we think of the $t$ as the ratio between the *mean difference* and the *standard error of the mean difference*. The $t$-statistic is different between the *two-sample* and the *paired* tests, but this does not necessarily mean that all elements of this ratio are also different. Indeed, if we look at the output above we can see that the *paired* test reports a mean difference of `-199.48` and the *two-sample* test reports the individual means as `200.56` and `400.04`. If we calculate the mean difference in the *two-sample* test we get "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43e3c02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] -199.48\n"
     ]
    }
   ],
   "source": [
    "print(200.56 - 400.04)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dba2cb",
   "metadata": {},
   "source": [
    "So, this is *identical* between the *paired* and *two-sample* tests. This should not be surprising, as we already established that repeated measurements do not affect the mean function. So, in either case, the groups means are the same, the mean difference is the same and the *numerator* of the $t$-statistic is the same. From this, we can conclude that the *difference* between the two methods concerns the *denominator* of the $t$-statistic. In other words, *the standard error of the difference changes under repeated measurements*.\n",
    "\n",
    "Given that we know the numerator for both tests, we can recover the denominators to see that this is the case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cf77d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]  7.808659 11.429554\n"
     ]
    }
   ],
   "source": [
    "mean.diff  <- -199.48\n",
    "paired.t   <- -25.546\n",
    "twosamp.t  <- -17.453\n",
    "paired.se  <-  mean.diff / paired.t\n",
    "twosamp.se <-  mean.diff / twosamp.t\n",
    "\n",
    "print(c(paired.se, twosamp.se))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0806ec6",
   "metadata": {},
   "source": [
    "So, the *standard error* of the difference is much *smaller* in the *paired* test when compared to the *two-sample* test. This should not be a surprise. Thinking back to our discussion from the beginning of the lesson, we know that the variance of the difference between two random variables should get *smaller* when they are positively correlated. From this, we can conclude that, when applied to *paired* data, the *two-sample* $t$-test is using a standard error that is *too large*. This tracks with everything we have discussed so far. However, the key question for us is *how* the *paired* $t$-test is able to do this. \n",
    "\n",
    "As mentioned at the start of this part of the lesson, there are two equivalent ways of thinking about this. We will discuss *both* below because their equivalence provides important information for conceptualising more complex methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95572e43",
   "metadata": {},
   "source": [
    "## The Model of *Paired Differences*\n",
    "The first method we can use is actually a bit of a *cheat* in order to side-step the issue of dependence entirely. However, conceptually, this is a really key step because it introduces the idea that we can correctly model repeated measurements by *removing something from the data*. In this first method, we remove it manually by subtracting the two repeats and analysing the *difference*. Further below we will see how this removal can be done *within the model itself*.\n",
    "\n",
    "... So the key insight is that when we take $D_{i} = y_{i1} - y_{i2}$, we have *removed* some element from the data that allows us to analyse the difference between the conditions without taking correlation into account. How is this possible? Precisely because, by subtracting these two values, we have *removed* the correlation. Thus, there is some shared component between $y_{i1}$ and $y_{i2}$ that cancels-out when we subtract them. In cancelling-out, the correlation is gone and we can treat $D_{i}$ as a vector of independent measurements. \n",
    "\n",
    "We can get more clarity on this by writing it formally. Let us *add* a shared component to the definition of both $y_{i1}$ and $y_{i2}$. We will call this $S_{i}$\n",
    "\n",
    "$$\n",
    "\\begin{alignat*}{1}\n",
    "    y_{i1} &= \\mu_{1} + S_{i} + \\epsilon_{i1} \\\\\n",
    "    y_{i2} &= \\mu_{2} + S_{i} + \\epsilon_{i2} \n",
    "\\end{alignat*}.\n",
    "$$\n",
    "\n",
    "So, the reason why $y_{i1}$ and $y_{i2}$ are correlated is because they *share* the same component $S_{i}$. This captures the idea that these measurements come the same *subject*. If we then *subtract* these values, $S_{i}$ will cancel-out\n",
    "\n",
    "$$\n",
    "\\begin{alignat*}{1}\n",
    "    (y_{i1} - y_{i2}) = D_{i} &= (\\mu_{1} - \\mu_{2}) + (S_{i} - S_{i}) + (\\epsilon_{i1} - \\epsilon_{i2}) \\\\\n",
    "                              &= \\mu^{(D)} + \\epsilon_{i}^{(D)}\n",
    "\\end{alignat*}.\n",
    "$$\n",
    "\n",
    "So, this tells us that $S_{i}$ precisely captures the correlation, because removing it renders the data *independent*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81ea497",
   "metadata": {},
   "source": [
    "## The Model of *Partitioned Errors*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a536102",
   "metadata": {},
   "source": [
    "### Partitioning the Error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeaf267",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "To begin with, let us just examine the overall variance of the data for the first 15 subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6e5e073",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "\u001b[1m\u001b[33mError\u001b[39m:\u001b[22m\n\u001b[33m!\u001b[39m object 'y.long' not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1m\u001b[33mError\u001b[39m:\u001b[22m\n",
      "\u001b[33m!\u001b[39m object 'y.long' not found\n",
      "\u001b[90m    \u001b[39m▆\n",
      "\u001b[90m 1. \u001b[39m├─\u001b[1mbase\u001b[22m::plot(...)\n",
      "\u001b[90m 2. \u001b[39m└─\u001b[1mgraphics\u001b[22m::plot.default(...)\n",
      "\u001b[90m 3. \u001b[39m  └─\u001b[1mgrDevices\u001b[22m::xy.coords(x, y, xlabel, ylabel, log)"
     ]
    }
   ],
   "source": [
    "\n",
    "subject <- rep(seq(1,50),each=2)\n",
    "subject <- as.factor(subject)\n",
    "\n",
    "plot(as.numeric(subject)[1:30],\n",
    "     y.long[1:30],\n",
    "     col=as.factor(cond),\n",
    "     xlab='Subject',\n",
    "     ylab='Y',\n",
    "     pch=16)\n",
    "\n",
    "abline(h=mean(y.long))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc17be77",
   "metadata": {},
   "source": [
    "Here we can see that there are 3 main sources of variation in these data:\n",
    "\n",
    "1. The two conditions have different means, thus measures from one condition tend to be larger than the other. In this example, the mean difference is only 0.21, so this is quite a subtle effect. But it is there. This is the *structured* variance we are interested in (can't remember what term was used for this?)\n",
    "2. The data come from different subjects, thus the degree to which the data fall above or below the grand means depends upon the individual subject. This is one source of *error variance* that explains why the data deviate from their expected value.\n",
    "3. Within each subject, there is variation between the two measures. The magnitude of this difference is sometimes bigger and sometimes smaller than the condition difference of 0.21. This is *another* source of *error variance*.\n",
    "\n",
    "So, we have one *structured* source of variation here (the difference between the experimental conditions), as well as *two* sources of error: the data come from different subjects and responses within each subject will also be different. The \"subject\" source captures the *internal consistency* of individual subjects (i.e. how correlated the responses are), whereas as the \"within-subject\" source captures the natural random variation we get between measurements, irrespective of whether they come from the same subject or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac50f1d0",
   "metadata": {},
   "source": [
    "To understand this more, we can work through *removing* each of these sources from the data to see how it can be decomposed. To begin with, we can remove the constant effect of each condition. To do so, we can simply take the residuals from the `two.sample.mod`, given that the predicted values from this model *are* the condition means. Once the effect of the conditions is removed, we have the plot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54649c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.two.sample <- resid(two.sample.mod)\n",
    "\n",
    "plot(as.numeric(subject)[1:30],\n",
    "     res.two.sample[1:30],\n",
    "     xlab='Subject',\n",
    "     ylab='Residuals',\n",
    "     col=as.factor(cond),\n",
    "     pch=16,\n",
    "     ylim=c(-3,3))\n",
    "\n",
    "abline(h=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796e3094",
   "metadata": {},
   "source": [
    "Now that we have removed any variation associated with the two conditions, we can turn to variability associated with the individual subjects. This is known as *between-subjects* variance. To remove it, we can calculate the mean value of each subject and then subtract it from the data. This results in the plot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f17bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.matrix <- matrix(res.two.sample, ncol=2, nrow=50, byrow=TRUE)\n",
    "sub.means  <- rowMeans(res.matrix)\n",
    "res.nosub  <- rep(0,100)\n",
    "sub.idx    <- as.numeric(subject)\n",
    "\n",
    "for (i in 1:100){\n",
    "    res.nosub[i] <- res.two.sample[i] - sub.means[sub.idx[i]]\n",
    "}\n",
    "\n",
    "plot(sub.idx[1:50],\n",
    "     res.nosub[1:50],\n",
    "     xlab='Subject',\n",
    "     ylab='Residuals - Subjects',\n",
    "     pch=16,\n",
    "     ylim=c(-3,3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b88629a",
   "metadata": {},
   "source": [
    "Notice that a huge amount of the variation in this data was attributable to the variation between different subjects. Now that this has been removed, we can effectively treat our data as *one big sample from a single subject*. As such, the variability we can see between all pairs of measurements provides us with an indication of how *internally consistent* a single subject is across the different conditions of the task. With the overall effect of the conditions removed, this remaining variability is not related to the conditions themseleves. Rather, it is related to other sources of random variation that cause an individual's response to change across multiple repeats of an experiment. This is known as the  *within-subject variance*. \n",
    "\n",
    "The variance associated with the two conditions is of direct interest because this captures our experimental effect of interest. Both the *between-subjects* and *within-subject* variance are effectively sources of *error* because they indicate different ways that the raw data may differ from the means of the conditions. One of these errors comes from the fact that different people may respond consistently higher or lower than the mean. The other comes from the fact that, even if an individual did not respond differently from the mean, natural variation across repeats will always be there.\n",
    "\n",
    "This partitioning of variance can be formally stated as\n",
    "\n",
    "$$\n",
    "\\text{Var}(y) = \\sigma^{2} = \\sigma^{2}_{b} + \\sigma^{2}_{w}.\n",
    "$$\n",
    "\n",
    "As such, we now have *three* choices when it comes to calculating standard errors. Do we use the pooled variance of $\\sigma^{2}$? The between-subjects variance of $\\sigma^{2}_{b}$, or the within-subject variance of $\\sigma^{2}_{w}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e411c2ba",
   "metadata": {},
   "source": [
    "Key Point\n",
    "In a regular paired t-test, the error variance consists of the differences between the means of the groups and the raw data. Howevever, when we have repeated measurements, this difference can be further divided into two sources ... This is consistent with the idea of the *between-subjects variance* $\\left(\\sigma^{2}_{b}\\right)$ and the *within-subject variance* $\\left(\\sigma^{2}_{w}\\right)$ ... As such, the difference with a *paired* test is that is uses the *within-subject variance* exclusively for determining the denominator of the $t$-statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44027d6f",
   "metadata": {},
   "source": [
    "### Partitioning the Error as a Decomposition of the Variance-covariance Matrix\n",
    "Now, we will connect what we have done above with the idea of modelling the variance-covariance matrix. Rather than doing this *explicitly*, the method above was an *implicit* modelling of the covariance structure..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273663a2-a58c-4565-a204-6a2ea79feb0b",
   "metadata": {},
   "source": [
    "### Explicit Two-sample $t$-test as a Linear Model\n",
    "To see this, we first start with the familiar case of the *two-sample* model. We can fit this as an LM within `R` as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbcad42-10d4-4d69-9d34-5ad829b7dca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.long <- as.vector(t(y)) # Turn y into a column\n",
    "cond   <- rep(c(\"A\",\"B\"),50) # Create a predictor for the two conditions\n",
    "\n",
    "two.sample.mod <- lm(y.long ~ cond)\n",
    "summary(two.sample.mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d54d7ae-2f80-4162-b1ad-ed66b71bbe90",
   "metadata": {},
   "source": [
    "Focussing on the coefficient and tests associated with `CondB` in the table, we can see $t = 0.928$ and $p = 0.356$, which is the same[^foot1] as we saw for the two-sample test earlier. We can also see that the degrees of freedom agree at $98$. So we have managed to successfully implement the *two-sample* $t$-test as a linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d3b644-71c1-4640-b0b3-28023e256ab2",
   "metadata": {},
   "source": [
    "Now, we know that this is *incorrect* for correlated data. However, specifying the model in this form allows us to dig deeper into *why* this is wrong, which will then provide us with the insight needed to correct this and thus tell us how the *paired* method is able to accommodate correlation. This will also provide us with the grounding needed to understand the traditional repeated measured ANOVA, as well as mixed-effect model a little later in the unit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affc7745-7055-4e16-83fa-4ce2d215b1ab",
   "metadata": {},
   "source": [
    "### Where Does the Standard Error Come From?\n",
    "To begin understanding what is going on here, we need to review where the value for the standard error comes from. In the context of a linear model, the standard error of $\\hat{\\beta}_{1}$ is given by\n",
    "\n",
    "$$\n",
    "\\text{SE}\\left(\\hat{\\beta}_{1}\\right) = \\sqrt{\\text{Var}\\left(\\hat{\\beta}_{1}\\right)} = \\sqrt{\\frac{\\hat{\\sigma}^{2}}{\\sum_{i=1}^{n}\\left(x_{i1} - \\bar{x}_{1}\\right)}}.\n",
    "$$\n",
    "\n",
    "So, the standard error is the square-root of the variance of an estimate, and the variance of an estimate is simply a scaled version of the error variance from the model. This scaling is not entirely clear when represented in the format above. However, when $x_{1}$ is simply a dummy variable encoding a mean difference, this simplifies to the known formula for the denominator of a $t$-test assuming equal variance in the two samples\n",
    "\n",
    "$$\n",
    "\\text{SE}\\left(\\hat{\\beta}_{1}\\right) = \\text{SE}\\left(\\hat{\\mu}_{1} - \\hat{\\mu}_{2}\\right) = \\sqrt{\\frac{\\hat{\\sigma}^{2}}{\\frac{1}{\\frac{1}{n_{1}} + \\frac{1}{n_{2}}}}} = \\sqrt{\\hat{\\sigma}^{2}\\left(\\frac{1}{n_{1}} + \\frac{1}{n_{2}}\\right)}.\n",
    "$$\n",
    "\n",
    "Here, we can more easily see that the standard error depends only upon the sample sizes and the error variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e2726e",
   "metadata": {},
   "source": [
    "`````{admonition} Key Point!\n",
    ":class: tip\n",
    "Under multiple repeats of the same experimnent, the sample sizes of the groups will remain the same. As such, this element of the standard error is simply a *constant scaling*. Whether the data are independent or not will not change this element of the standard error because the formula is *always the same*. The only element that can change is the *error variance*. As such, this must be the source of the difference between the *two-sample* and *paired* approaches.\n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6733cb0",
   "metadata": {},
   "source": [
    "We can verify that this formula for the standard error is correct in the *two-sample* model by calculating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5364a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma2 <- summary(two.sample.mod)$sigma^2\n",
    "sqrt(sigma2 * (1/50 + 1/50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7e5ff1",
   "metadata": {},
   "source": [
    "which agrees with out results so far. As the element of most interest here is the estimate of $\\sigma^{2}$, the next obvious question is where does this come from? \n",
    "\n",
    "As a review, the error variance in a linear model is estimated using\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^{2} = \\frac{\\sum_{i=1}^{n}\\epsilon_{i}^{2}}{n-p},\n",
    "$$\n",
    "\n",
    "which is also known as the *residual mean square* or *error mean square*. We can again verify this for our example by calculating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9558e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma2\n",
    "\n",
    "sum(resid(two.sample.mod)^2) / two.sample.mod$df.residual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aba2b44",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "As such, if this is the element that differs between the *two-sample* and *paired* model, then our final suspect must be the *model residuals*. More specifically, the residuals must be *larger* in the *two-sample* case and *smaller* in the *paired* case. This is the only way the standard errors can differ. But this still does not explain *why* this is the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f925e67-aa9b-409f-a3c0-d3ef395d262a",
   "metadata": {},
   "source": [
    "### Recreating the Paired $t$-test in the Linear Model\n",
    "\n",
    "From all we discussed earlier, the aim is therefore to *remove* the between-subject variance from the residuals so that the error variance of the model only contains the *within-subject variance*. If we do not do this, then $\\sigma^{2} = \\sigma^{2}_{b} + \\sigma^{2}_{w}$, which will be too large to accurately capture the standard error of the mean difference under repeated measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32e1c6a-3b01-4277-a7d9-23be4ac7754e",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(car)\n",
    "\n",
    "subject <- rep(seq(1,50),each=2)\n",
    "subject <- as.factor(subject)\n",
    "\n",
    "paired.mod <- lm(y.long ~ cond + subject)\n",
    "summary(paired.mod)\n",
    "\n",
    "print(Anova(paired.mod))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e4c10f-0528-48e3-ac86-141aaef3d95f",
   "metadata": {},
   "source": [
    "Now, the output here is a bit of mess due to all the subject effects. However, if you look at the coefficient and test for `CondB`, notice that $t = 2.495$ and $p = 0.016$, which is the same as the *paired* $t$-test from earlier. Furthermore, the degrees of freedom are now correct at $49$. As such, adding the subject effects to the model has allowed the *between-subjects* error to be partitioned out and thus the remaining variance calculated from the residuals is *only* the *within-subject* error. This is the error needed to correctly estimate the standard error of the paired difference and thus the model results are now correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe9ee4b-b274-4207-b836-2097bec1c194",
   "metadata": {},
   "source": [
    "```{admonition} Advanced: Understanding the coefficients in the paired model\n",
    ":class: warning, dropdown\n",
    "So from this, we can see that\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mu_{1} &= \\beta_{0} + \\frac{1}{n}\\sum_{k=2}^{n+1}\\beta_{k} \\\\\n",
    "\\mu_{2} &= \\beta_{0} + \\beta_{1} + \\frac{1}{n}\\sum_{k=2}^{n+1}\\beta_{k}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Solving for both $\\beta_{0}$ and $\\beta_{1}$ gives\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\beta_{0} &= \\mu_{1} - \\frac{1}{n}\\sum_{k=2}^{n+1}\\beta_{k} \\\\\n",
    "\\beta_{1} &= \\beta_{0} + \\frac{1}{n}\\sum_{k=2}^{n+1}\\beta_{k} - \\mu_{2} = \\mu_{1} - \\mu_{2}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "So, rather unituitively, the intercept is actually the mean of the first group, minus the average of the subject effects. This then raises the question of what exactly the subject effects are? We can use a similar approach as above to solve for these. For instance, the expected value of the response from subject 2 in condition A is\n",
    "\n",
    "$$\n",
    "\\mu_{12} = \\beta_{0} + \\beta_{2}.\n",
    "$$\n",
    "\n",
    "Meaning that the effect for subject 2 is\n",
    "\n",
    "$$\n",
    "\\beta_{2} = \\mu_{12} - \\beta_{0}.\n",
    "$$\n",
    "\n",
    "As such, the subject effects are effectively *residuals* around the intercept. Importantly, these are *constant offsets*, irrespective of the condition. For instance. As such, subject 2 is expect to lie the same distance from the mean of condition A and the mean of condition B.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8caa26e",
   "metadata": {},
   "source": [
    "## Section Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a091fc2-fa8d-4da2-b4af-83c5c5c7089f",
   "metadata": {},
   "source": [
    "[^foot2]: Here we set the variance to be the same between the samples because this (a) agrees with the simulations and (b) prevents conflation of the issue of correlation with conflation of the issue of homogeneity of variance.\n",
    "\n",
    "[^foot1]: Due to the way the factors are coded in `R`, the coefficient is actually the opposite comparison here, hence why the $t$-statistic is *positive* rather than *negative*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4892ba28",
   "metadata": {},
   "source": [
    "[^paired-foot]: Paired data simply means data where each experimental unit provides a *pair* of measurements. For instance, each subject takes part in *two* conditions, or is measured at *two* separate time points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33eea82",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "name": "r"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
